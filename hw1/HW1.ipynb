{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by downloading 20-newsgroup text dataset:\n",
    "\n",
    "```http://scikit-learn.org/stable/datasets/index.html#the-20-newsgroups-text-dataset```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Only lowercase preprocessing\n",
    "'''\n",
    "# tokenizer = spacy.load('en_core_web_sm')\n",
    "# punctuations = string.punctuation\n",
    "# # lowercase and remove punctuation\n",
    "# def tokenize(sent):\n",
    "#     tokens = tokenizer(sent)\n",
    "#     return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "# def tokenize_data(mode, pn):\n",
    "#     if pn == \"neg\":\n",
    "#         label = 0\n",
    "#     else:\n",
    "#         label = 1\n",
    "#     token_train = pd.DataFrame(columns = [\"label\", \"content\"])\n",
    "#     for file in glob.glob(\"{0}/{1}/*\".format(mode, pn)):\n",
    "#         with open (file, 'r') as f:\n",
    "#             content = []\n",
    "#             for line in f:\n",
    "#                 content += tokenize(line)\n",
    "#             token_train = token_train.append({\"label\": label, \"content\": content}, ignore_index=True)  \n",
    "#     return token_train\n",
    "# # neg_train = tokenize_data('train', 'neg')\n",
    "# # pos_train = tokenize_data('train', 'pos')\n",
    "# # neg_test = tokenize_data('test', 'neg')\n",
    "# # pos_test = tokenize_data('test', 'pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Save the data\n",
    "'''\n",
    "# total_data = pd.concat([neg_train, pos_train])\n",
    "# total_data['index'] = range(len(total_data))\n",
    "# total_data = total_data.set_index('index')\n",
    "# train_split = 0.2\n",
    "# train_data, val_data = train_test_split(total_data, test_size = train_split, random_state=42)\n",
    "# train_data.to_csv(\"new_train.csv\")\n",
    "# val_data.to_csv(\"new_val.csv\")\n",
    "# test_data = pd.concat([neg_test, pos_test])\n",
    "# test_data['index'] = range(len(test_data))\n",
    "# test_data = test_data.set_index('index')\n",
    "# test_data.to_csv(\"new_test.csv\")\n",
    "train_data = pd.read_csv(\"new_train.csv\")\n",
    "val_data = pd.read_csv(\"new_val.csv\")\n",
    "test_data = pd.read_csv(\"new_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "move stop words and useless token preprocessing \n",
    "'''\n",
    "# tokenizer = spacy.load('en_core_web_sm')\n",
    "# stop_words = set(stopwords.words('english')) \n",
    "# stop_words.update(['\\'s', '\\'t'])\n",
    "# punctuations = string.punctuation\n",
    "# # lowercase and remove punctuation\n",
    "# def tokenize(sent):\n",
    "#     tokens = tokenizer(sent)\n",
    "#     return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "# def tokenize_data(mode, pn):\n",
    "#     if pn == \"neg\":\n",
    "#         label = 0\n",
    "#     else:\n",
    "#         label = 1\n",
    "#     token_train = pd.DataFrame(columns = [\"label\", \"content\"])\n",
    "#     for file in tqdm_notebook(glob.glob(\"{0}/{1}/*\".format(mode, pn))):\n",
    "#         with open (file, 'r') as f:\n",
    "#             content = []\n",
    "#             for line in f:\n",
    "#                 new_line = re.sub(r'<br />', '', line)\n",
    "#                 tokens = tokenize(new_line)\n",
    "#                 tokens = [w for w in tokens if not w in stop_words] \n",
    "#                 content += tokens\n",
    "#             token_train = token_train.append({\"label\": label, \"content\": content}, ignore_index=True)  \n",
    "#     return token_train\n",
    "# neg_train = tokenize_data('train', 'neg')\n",
    "# pos_train = tokenize_data('train', 'pos')\n",
    "# neg_test = tokenize_data('test', 'neg')\n",
    "# pos_test = tokenize_data('test', 'pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Save the data for new schem\n",
    "'''\n",
    "# total_data = pd.concat([neg_train, pos_train])\n",
    "# total_data['index'] = range(len(total_data))\n",
    "# total_data = total_data.set_index('index')\n",
    "# train_split = 0.2\n",
    "# train_data, val_data = train_test_split(total_data, test_size = train_split, random_state=42)\n",
    "# train_data.to_csv(\"new_train_sche.csv\")\n",
    "# val_data.to_csv(\"new_val_sche.csv\")\n",
    "# test_data = pd.concat([neg_test, pos_test])\n",
    "# test_data['index'] = range(len(test_data))\n",
    "# test_data = test_data.set_index('index')\n",
    "# test_data.to_csv(\"new_test_sche.csv\")\n",
    "\n",
    "# train_data = pd.read_csv(\"new_train_sche.csv\")\n",
    "# val_data= pd.read_csv(\"new_val_sche.csv\")\n",
    "# test_data = pd.read_csv(\"new_test_sche.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# Split train data into actual train and validation sets\n",
    "# train_data = pd.read_csv(\"new_train.csv\")\n",
    "# val_data = pd.read_csv(\"new_val.csv\")\n",
    "# test_data = pd.read_csv(\"new_test.csv\")\n",
    "print (\"Train dataset size is {}\".format(len(train_data)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###N-gram \n",
    "def ngrams(data, n):\n",
    "    output = []\n",
    "    for i in range(len(data)-n+1):\n",
    "        output.append(data[i:i+n])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_result(dataset, n):\n",
    "    token_dataset = []\n",
    "    all_token = []\n",
    "    for i in dataset.index:\n",
    "        gram_result = []\n",
    "        for j in range(n):\n",
    "            n_gram_result = ngrams(ast.literal_eval(dataset.content[i]), j+1)\n",
    "            gram_result += [\" \".join(n)for n in n_gram_result]\n",
    "        token_dataset.append(gram_result)\n",
    "        all_token += gram_result\n",
    "    return token_dataset, all_token\n",
    "\n",
    "train_data['ngram'], all_token = ngrams_result(train_data, 1)\n",
    "val_data['ngram'], _= ngrams_result(val_data, 1)\n",
    "test_data['ngram'], _ = ngrams_result(test_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['ngram'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to create the vocabulary of most common 10,000 tokens in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 40000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 35959 ; token sanitation\n",
      "Token sanitation; token id 35959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40002"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))\n",
    "len(id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(dataset):\n",
    "    indices_data = []\n",
    "    for i in dataset.index:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in dataset.ngram[i]]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data)\n",
    "train_data['indices'] = train_data_indices\n",
    "\n",
    "val_data_indices = token2index_dataset(val_data)\n",
    "val_data['indices'] = val_data_indices\n",
    "\n",
    "test_data_indices = token2index_dataset(test_data)\n",
    "test_data['indices'] = test_data_indices\n",
    "# val_data_indices = token2index_dataset(val_data_tokens)\n",
    "# test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))\n",
    "# train_data.to_csv(\"ngram_train.csv\")\n",
    "# val_data.to_csv(\"n_gram_val.csv\")\n",
    "# test_data.to_csv(\"n_gram_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create PyTorch DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 400\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data.indices, train_data.label)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data.indices, val_data.label)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data.indices, test_data.label)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define Bag-of-Words model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,2)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "'''\n",
    "for learning rate decay \n",
    "'''\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.99)\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        ##check if the prediction is correct and get the data\n",
    "        #print(data[0], predicted[0], labels.view_as(predicted)[0])\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "validation vector for new scheme\n",
    "'''\n",
    "#validation_acc_sche = []\n",
    "#validation_acc = []\n",
    "\n",
    "'''\n",
    "validation vector for grams \n",
    "'''\n",
    "#uni_gram_acc = []\n",
    "#bi_gram_acc = []\n",
    "#tri_gram_acc = []\n",
    "#four_gram_acc = []\n",
    "\n",
    "'''\n",
    "validatoin vector for vocabulary size\n",
    "'''\n",
    "#max_5000 = []\n",
    "#max_10000 = uni_gram_acc\n",
    "#max_20000 = []\n",
    "#max_40000 = []\n",
    "\n",
    "'''\n",
    "embedding_size\n",
    "'''\n",
    "#embed_50 = []\n",
    "#embed_100 = []\n",
    "#embed_150 = []\n",
    "#embed_200 = []\n",
    "\n",
    "\n",
    "'''\n",
    "Optimizer\n",
    "'''\n",
    "#sgd_acc = []\n",
    "#adam_acc = []\n",
    "\n",
    "'''\n",
    "Learning Rate\n",
    "'''\n",
    "#lr_001 = []\n",
    "#lr_0001 = []\n",
    "#lr_00005 = []\n",
    "\n",
    "'''\n",
    "Learning Rate Linear Decay\n",
    "'''\n",
    "#lr_decay = []\n",
    "# iterations = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):        \n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #learning rate decay\n",
    "#         if i > 0 and i % 30 == 0:\n",
    "#             scheduler.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            lr_decay.append(val_acc)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 10 epochs\n",
      "Val Acc 89.1\n",
      "Test Acc 86.684\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scheme choose\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot([i+1 for i in range(len(validation_acc))], validation_acc, label = \"lower case only\")\n",
    "plt.plot([i+1 for i in range(len(validation_acc_sche))], validation_acc_sche, label = \"lower case + remove stopwords & special token\")\n",
    "plt.xlabel(\"Batchs for every 100 iterations\")\n",
    "plt.ylabel(\"Validate accuracy for every 100 iterations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Gram choose  \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot([i+1 for i in range(len(uni_gram_acc))], uni_gram_acc, label = \"Unit gram\")\n",
    "plt.plot([i+1 for i in range(len(bi_gram_acc))], bi_gram_acc, label = \"2 gram\")\n",
    "plt.plot([i+1 for i in range(len(tri_gram_acc))], tri_gram_acc, label = \"3 gram\")\n",
    "plt.plot([i+1 for i in range(len(four_gram_acc))], four_gram_acc, label = \"4 gram\")\n",
    "plt.xlabel(\"Batchs for every 100 iterations\")\n",
    "plt.ylabel(\"Validate accuracy for every 100 iterations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Vocabular size \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot([i+1 for i in range(len(max_5000))], max_5000, label = \"5000 Vocabulary\")\n",
    "plt.plot([i+1 for i in range(len(max_10000))], max_10000, label = \"10000 Vocabulary\")\n",
    "plt.plot([i+1 for i in range(len(max_20000))], max_20000, label = \"20000 Vocabulary\")\n",
    "plt.plot([i+1 for i in range(len(max_40000))], max_40000, label = \"40000 Vocabulary\")\n",
    "plt.xlabel(\"Batchs for every 100 iterations\")\n",
    "plt.ylabel(\"Validate accuracy for every 100 iterations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Embedding size \n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot([i+1 for i in range(len(embed_100))], embed_100, label = \"embedding size 100\")\n",
    "plt.plot([i+1 for i in range(len(embed_200))], embed_200, label = \"embedding size 200\")\n",
    "plt.plot([i+1 for i in range(len(embed_150))], embed_150, label = \"embedding size 150\")\n",
    "plt.xlabel(\"Batchs for every 100 iterations\")\n",
    "plt.ylabel(\"Validate accuracy for every 100 iterations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Optimizaer\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot([i+1 for i in range(len(sgd_acc))], sgd_acc, label = \"SGD\")\n",
    "plt.plot([i+1 for i in range(len(adam_acc))], adam_acc, label = \"Adam\")\n",
    "plt.xlabel(\"Batchs for every 100 iterations\")\n",
    "plt.ylabel(\"Validate accuracy for every 100 iterations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fixed learning rate and Learning rate decay\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot([i+1 for i in range(len(lr_001))], lr_001, label = \"0.01\")\n",
    "plt.plot([i+1 for i in range(len(lr_0001))], lr_0001, label = \"0.001\")\n",
    "plt.plot([i+1 for i in range(len(lr_00005))], lr_00005, label = \"0.0005\")\n",
    "plt.plot([i+1 for i in range(len(lr_decay))], lr_decay, label = \"learning decay\")\n",
    "plt.xlabel(\"Batchs for every 100 iterations\")\n",
    "plt.ylabel(\"Validate accuracy for every 100 iterations\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The best model has the combination, Lower_case only, 1 gram, Vocabulary size = 40000, \n",
    "embedding_size = 100, optimizer = Adam, learning_rate = 0.001, epochs = 10\n",
    "'''\n",
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "# print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert token to id\"\"\"\n",
    "new_dict = dict((v,k) for k,v in token2id.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "map id to token\n",
    "\n",
    "data is in the fail_correct_data.py\n",
    "\"\"\"\n",
    "\n",
    "##Incorrect (3 example)\n",
    "new = []\n",
    "for i in fail1:\n",
    "    new += [new_dict[i]]\n",
    "print(\" \".join(new), '\\n')\n",
    "\n",
    "\n",
    "new = []\n",
    "for i in fail2:\n",
    "    new += [new_dict[i]]\n",
    "print(\" \".join(new), '\\n')\n",
    "\n",
    "new = []\n",
    "for i in fail3:\n",
    "    new += [new_dict[i]]\n",
    "print(\" \".join(new), '\\n')\n",
    "\n",
    "##Correct(3 example)\n",
    "new = []\n",
    "for i in correct1:\n",
    "    new += [new_dict[i]]\n",
    "print(\" \".join(new) + '\\n')\n",
    "\n",
    "new = []\n",
    "for i in correct2:\n",
    "    new += [new_dict[i]]\n",
    "print(\" \".join(new), '\\n')\n",
    "\n",
    "new = []\n",
    "for i in correct3:\n",
    "    new += [new_dict[i]]\n",
    "print(\" \".join(new))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Incorrect:\n",
    "\n",
    "it 's the <unk> starring reed hadley with an all star guest cast the film begins with an accidental convenient kidnapping which leads to one thing and another which does n't really indicate the main story which is a big house <unk> prison break story the story is very improbable to say the least it 's like a tv show only more violent for the <unk> /><br />but the cast is a trip picture this ralph <unk> is sent to prison his cell mates are the following criminals <unk> crawford <unk> <unk> jr. charles bronson reading a muscle magazine and william <unk> reading a detective magazine honest you should know that an early scene reveals what happens to the missing boy <unk> the ending <unk> if you do n't want to have that hanging do n't miss the opening scenes between the <unk> and the boy peter <unk> doing well as a runaway <unk> br /><br big house <unk> 1955 howard w. \n",
    "\n",
    "this film got terrible reviews but because it was offbeat and because critics do n't usually get offbeat films i thought i 'd give it a try unfortunately they were largely right in this <unk> /><br />the film just has an awkward feel too it that is most off putting the sort of feel that is impossible to describe but it 's not a good one to further <unk> things the script is a dull <unk> thing that is only vaguely interesting.<br /><br />the immensely talented thurman just <unk> through this mess creating barely an impact hurt and <unk> try in vain to add something to the film with enthusiastic performance but there is nothing in the script it may have been less embarrassing for them if they had merely chosen to drift and get it over with like <unk> /><br />one thing the <unk> film critics did fail to mention however is that the film is actually quite funny whether it be moments of accurate satire or some outrageously weird moments like when the <unk> in question chase hurt off their ranch with the smell of their <unk> ... <unk> ... front <unk> /><br />because of the <unk> <unk> throughout while i would n't recommend this film there is entertainment to be had and watching even <unk> get the blues is worthwhile for something different\n",
    "\n",
    "director warren beatty 's intention to turn <unk> <unk> 's famous comic strip into a live action cartoon with beatty himself cast in the lead as the square <unk> detective had sweet <unk> of innocent nostalgia -- quite unusual and intriguing coming from warren beatty unfortunately the picture is <unk> ham fun for awhile but eventually <unk> dick tracy attempts to bring down mobster big boy <unk> aided by loving tess <unk> but <unk> up by evil <unk> <unk> for the first half hour or so the oscar winning art direction and set design are wonderful to <unk> but as the plot <unk> along predictably with no real <unk> in the writing things begin to <unk> al pacino got a surprise supporting oscar nomination as bad boy <unk> and madonna who is mostly used as a <unk> prop gets to sing stephen <unk> 's sooner or later i always get my man which <unk> the award for best original song lots of heart thanks to beatty -- who was dedicated to his vision -- but the picture is too cool and <unk> it lacks heat 1/2 from"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Correct:\n",
    "\n",
    "what a mess why was this movie made this and other movies of its caliber should be teaching tools on how not to make a movie children may like it but anyone over 10 may or will <unk> to make matters worse was the fact that such great talent like whoopi goldberg and <unk> <unk> <unk> were entirely wasted in a film <unk> of any notice\n",
    "\n",
    "i almost saw this at an actual movie theatre an art house theatre no less but could n't make it there in the one whole week it played but yesterday i finally saw it on cable and ... well ... i was n't disappointed that 's for sure madonna has done it again yet another bomb when will this woman learn when will the studios learn or perhaps they already have since this film was largely dumped with little <unk> and deadly word of mouth one would hope that being directed by her talented husband who 's created some interesting and/or terribly entertaining work would bring out the same quality madonna showed in desperately seeking susan alas it just is n't meant to be for here she is at her very worst <unk> convinced of her own greatness the <unk> <unk> every frame she 's in made all the more unbearable by her <unk> faux british accent an accent that only <unk> the fact that her speaking voice is immature in quality and not especially pleasant this may sound unnecessarily cruel but listen to the woman and look at her films of say the past decade like a latter day bette davis there is an <unk> <unk> to not only her <unk> but to her very face and body which here despite the warm photography displayed throughout the film perhaps its only saving grace are done no <unk> to her credit the entire affair is so <unk> that one wonders if the world 's greatest actress on her best day could do anything with this mess no one involved escapes <unk> bruce <unk> actually seems <unk> to be on screen though poor <unk> <unk> seems to carry herself as if she 's actually in something good which had me thinking all the while denial ai n't just a river in egypt <unk> <unk> son of <unk> <unk> star of the italian original swept away ... is like his father before him immensely attractive and is n't altogether bad despite winning a <unk> nomination for worst actor but like almost everything else about this production it all comes back to madonna on whose shoulders rest the blame why her why not her husband director guy ritchie just who do you think was behind this remake what actress would n't want nearly every shot of a movie to be centered on her \n",
    "\n",
    "this is a very very early bugs bunny cartoon as a result the character is still in a transition period -- he is not drawn as <unk> as he later was and his voice is n't quite right in addition the chemistry between <unk> and bugs is a little unusual <unk> is some poor <unk> who buys bugs from a pet shop -- there is no gun or desire on his part to blast the bunny to <unk> however despite this this is still a very enjoyable film the early bugs was definitely more sassy and cruel than his later <unk> in later films he messed with <unk> <unk> sam and others because they started it -- they messed with the rabbit but in this film he is much more like <unk> duck of the late 30s and early 40s -- a jerk who just loves irritating others a true <unk> instead of the hero of the later cartoons while this is n't among the best bug bunny cartoons it sure is fun to watch and it 's interesting to see just how much he 's changed over the years"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
